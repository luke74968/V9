# Copyright (c) 2025 Minuk Lee. All rights reserved.
# 
# This source code is proprietary and confidential.
# Unauthorized copying of this file, via any medium is strictly prohibited.
# 
# For licensing terms, see the LICENSE file.
# Contact: minuklee@snu.ac.kr
# 
import json
import torch
from torch.utils.data import Dataset
from tensordict import TensorDict
from tqdm import tqdm
from typing import Tuple, List, Dict, Any

from .solver_env import PocatEnv
from .env_generator import PocatGenerator

def expert_collate_fn(batch: List[Tuple[TensorDict, torch.Tensor]]) -> Tuple[TensorDict, torch.Tensor]:
    """
    TensorDictì™€ Tensorë¡œ êµ¬ì„±ëœ ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸ë¥¼
    í•˜ë‚˜ì˜ ìŠ¤íƒ(Stacked) í…ì„œë¡œ ë¬¶ëŠ” ì»¤ìŠ¤í…€ collate í•¨ìˆ˜ì…ë‹ˆë‹¤.
    
    Args:
        batch: [(TensorDict_1, Tensor_1), (TensorDict_2, Tensor_2), ...]
        
    Returns:
        (Stacked_TensorDict, Stacked_Tensor)
    """
    
    # 1. íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë‘ ê°œì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬
    td_list = [item[0] for item in batch]
    reward_list = [item[1] for item in batch]
    
    # 2. TensorDict ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ TensorDictë¡œ ìŠ¤íƒ
    # (B=1, N_max, D) -> (B=batch_size, N_max, D)
    batched_tds = torch.stack(td_list, dim=0)
    
    # 3. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ë¥¼ (B, 1) í…ì„œë¡œ ìŠ¤íƒ
    batched_rewards = torch.stack(reward_list, dim=0)
    
    return batched_tds, batched_rewards


class ExpertReplayDataset(Dataset):
    """
    "ì •ë‹µì§€" JSON íŒŒì¼ì„ ë¡œë“œí•˜ê³ , í™˜ê²½ì—ì„œ ë¦¬í”Œë ˆì´(Replay)í•˜ì—¬
    (State, Final_Reward) í˜ì–´(Pair)ë¥¼ ìƒì„±í•˜ëŠ” ì§€ë„í•™ìŠµìš© ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.
    
    (Critic ì‚¬ì „í›ˆë ¨ìš©)
    """
    def __init__(self, 
                 expert_data_path: str, 
                 env: PocatEnv, 
                 device: str = "cpu",
                 N_max: int = 500):
        """
        Args:
            expert_data_path (str): "expert_data.json" íŒŒì¼ ê²½ë¡œ.
            env (PocatEnv): ë¦¬í”Œë ˆì´ë¥¼ ì‹¤í–‰í•  í™˜ê²½ ì¸ìŠ¤í„´ìŠ¤.
            device (str): í…ì„œ ë””ë°”ì´ìŠ¤.
            N_max (int): í™˜ê²½ì˜ N_MAX ê°’.
        """
        self.env = env
        self.device = device
        self.N_max = N_max
        self.replay_buffer: List[Tuple[TensorDict, torch.Tensor]] = []

        print(f"\nğŸ§  'ì •ë‹µì§€' ë¦¬í”Œë ˆì´ ë°ì´í„°ì…‹ ìƒì„± ì¤‘ (Critic Pre-train)...")
        print(f"   - ì •ë‹µì§€ íŒŒì¼ ë¡œë“œ: {expert_data_path}")
        
        try:
            with open(expert_data_path, 'r', encoding='utf-8') as f:
                expert_traces = json.load(f)
            if not isinstance(expert_traces, list):
                expert_traces = []
        except Exception as e:
            print(f"âŒ 'ì •ë‹µì§€' íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            expert_traces = []

        # (config.json íŒŒì¼ë³„ë¡œ Generatorë¥¼ ìºì‹±)
        generator_cache: Dict[str, PocatGenerator] = {}

        pbar = tqdm(expert_traces, desc="   - OR-Tools ê²½ë¡œ ë¦¬í”Œë ˆì´ ì¤‘")
        for trace in pbar:
            try:
                config_file = trace["config_file"]
                target_reward = trace["target_reward"]
                # ì•¡ì…˜ ì‹œí€€ìŠ¤ (Parameterized Action ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸)
                action_sequences: List[List[Dict[str, Any]]] = trace["action_sequences"]
                
                # 1. ì •ë‹µì§€ì™€ ë™ì¼í•œ configë¡œ Generator ì¤€ë¹„
                if config_file not in generator_cache:
                    generator_cache[config_file] = PocatGenerator(
                        config_file_path=config_file,
                        N_max=self.N_max
                    )
                generator = generator_cache[config_file]
                
                # (B=1, 1) í¬ê¸°ì˜ ì •ë‹µ ë³´ìƒ í…ì„œ
                target_reward_tensor = torch.tensor([[target_reward]], dtype=torch.float32, device=self.device)

                # 2. ëª¨ë“  ê²½ë¡œ(Load)ë¥¼ ìˆœíšŒ
                for path_actions in action_sequences:
                    # 3. í™˜ê²½ ë¦¬ì…‹ (B=1)
                    td_initial = generator(batch_size=1).to(self.device)
                    td = self.env._reset(td_initial) # (N_MAX í¬ê¸°ì˜ ìƒíƒœ)
                    
                    # 4. 'ì •ë‹µì§€'ì˜ ì•¡ì…˜ì„ í•œ ìŠ¤í…ì”© ë¦¬í”Œë ˆì´
                    for action_dict in path_actions:
                        
                        # (A) ë¦¬í”Œë ˆì´: í˜„ì¬ ìƒíƒœ(td)ì™€ ì •ë‹µ ë³´ìƒ(target_reward)ì„ ë²„í¼ì— ì €ì¥
                        # .clone()ìœ¼ë¡œ í…ì„œì˜ í˜„ì¬ ìŠ¤ëƒ…ìƒ·ì„ ì €ì¥
                        self.replay_buffer.append((
                            td.clone().squeeze(0), # (1,N,D) -> (N,D)
                            target_reward_tensor.clone().squeeze(0) # (1,1) -> (1,)
                        ))
                        
                        # (B) ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ ì´ë™ (ì•¡ì…˜ ë”•ì…”ë„ˆë¦¬ë¥¼ í…ì„œë¡œ ë³€í™˜)
                        action_tensor_dict = {
                            "action_type": torch.tensor([[action_dict["action_type"]]], device=self.device),
                            "connect_target": torch.tensor([[action_dict["connect_target"]]], device=self.device),
                            "spawn_template": torch.tensor([[action_dict["spawn_template"]]], device=self.device),
                        }
                        
                        td.set("action", action_tensor_dict)
                        td = self.env.step(td)["next"]
                        
                        if td["done"].item():
                            break # (ê²½ë¡œ ì™„ì„± ë˜ëŠ” ì‹¤íŒ¨)
                            
            except Exception as e:
                print(f"âŒ ë¦¬í”Œë ˆì´ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (Config: {trace.get('config_file', 'N/A')}): {e}")
                
        if not self.replay_buffer:
            print("âš ï¸ ê²½ê³ : 'ì •ë‹µì§€' ë¦¬í”Œë ˆì´ ê²°ê³¼, ìœ íš¨í•œ (ìƒíƒœ, ë³´ìƒ) ë°ì´í„°ê°€ 0ê°œì…ë‹ˆë‹¤.")
        else:
            print(f"âœ… 'ì •ë‹µì§€' ë¦¬í”Œë ˆì´ ì™„ë£Œ. ì´ {len(self.replay_buffer)}ê°œì˜ (ìƒíƒœ, ë³´ìƒ) í˜ì–´ ìƒì„±.")

    def __len__(self) -> int:
        return len(self.replay_buffer)

    def __getitem__(self, idx: int) -> Tuple[TensorDict, torch.Tensor]:
        """ ë²„í¼ì—ì„œ (State, Reward) í˜ì–´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. """
        # (TensorDict[N_max, D], Tensor[1,])
        return self.replay_buffer[idx]