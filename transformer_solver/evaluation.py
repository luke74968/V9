import torch
import numpy as np
import time
from tqdm import tqdm
from typing import Dict, Any, Tuple
from tensordict import TensorDict

# ì‚¬ìš©ì í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from transformer_solver.solver_env import PocatEnv, FAILURE_PENALTY

class PocatEvaluator:
    def __init__(self, env: PocatEnv, model: torch.nn.Module, device: str):
        self.env = env
        self.model = model
        self.device = device
        self.model.eval()
        self.model.to(device)

    def evaluate(self, 
                 dataset: TensorDict, 
                 batch_size: int = 16, 
                 decode_type: str = "greedy",
                 pomo_sampling: bool = True) -> Dict[str, Any]:
        """
        ë°ì´í„°ì…‹ì— ëŒ€í•´ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê³  í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            dataset: í‰ê°€í•  ë¬¸ì œë“¤ì´ ë‹´ê¸´ TensorDict (ì „ì²´ ë°ì´í„°)
            batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  ë°°ì¹˜ í¬ê¸°
            decode_type: 'greedy' or 'sampling'
            pomo_sampling: Trueì¼ ê²½ìš° POMO(Multi-start) ì ìš©, Falseë©´ ë‹¨ì¼ ì‹¤í–‰
        """
        
        # í†µê³„ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
        stats = {
            "total_instances": 0,
            "feasible_instances": 0,  # êµ¬ì¡°ì /ë¬¼ë¦¬ì ìœ¼ë¡œ ì„±ê³µí•œ ì¼€ì´ìŠ¤
            "optimal_rewards": [],    # ê° ë°°ì¹˜ì˜ Best Reward
            "bom_costs": [],          # ì„±ê³µí•œ ì¼€ì´ìŠ¤ì˜ BOM Cost
            "sleep_costs": [],        # ì„±ê³µí•œ ì¼€ì´ìŠ¤ì˜ Sleep Penalty Cost
            "inference_times": [],
            "avg_starts": 0,          # í‰ê·  POMO ì‹œë„ íšŸìˆ˜
        }

        # ë°ì´í„°ì…‹ ë¶„í•  (Manual Batching using TensorDict)
        total_items = dataset.batch_size[0]
        num_batches = (total_items + batch_size - 1) // batch_size
        
        print(f"ğŸš€ Starting Evaluation: {total_items} instances ({num_batches} batches)")
        
        for i in tqdm(range(num_batches), desc="Evaluating"):
            # 1. ë°°ì¹˜ ìŠ¬ë¼ì´ì‹±
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, total_items)
            # clone()í•˜ì—¬ ì›ë³¸ ë³´ì¡´ ë° ë””ë°”ì´ìŠ¤ ì´ë™
            batch_td = dataset[start_idx:end_idx].clone().to(self.device)
            current_batch_size = end_idx - start_idx
            
            # 2. POMO ì„¤ì • í™•ì¸ (ì‹œì‘ ë…¸ë“œ ê°œìˆ˜ í™•ì¸)
            # í™˜ê²½ì—ì„œ ê°€ëŠ¥í•œ ì‹œì‘ì (Load) ê°œìˆ˜ë¥¼ ê°€ì ¸ì˜´
            num_starts, _ = self.env.select_start_nodes(batch_td)
            
            if not pomo_sampling:
                num_starts = 1 # ê°•ì œ ë‹¨ì¼ ì‹¤í–‰ (í•„ìš” ì‹œ ë¡œì§ ìˆ˜ì • í•„ìš”)

            start_time = time.time()
            
            with torch.no_grad():
                # 3. ëª¨ë¸ ì¶”ë¡  (PocatModel.forward ë‚´ë¶€ì—ì„œ POMO í™•ì¥ ë° ë£¨í”„ ì²˜ë¦¬)
                # model.forward()ëŠ” {reward, actions, bom_cost, sleep_cost...} ë°˜í™˜
                # ë°˜í™˜ëœ í…ì„œì˜ í¬ê¸°: (Batch_Size * Num_Starts, ...)
                result = self.model(
                    batch_td, 
                    self.env, 
                    decode_type=decode_type,
                    return_final_td=False # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ False
                )
                
            elapsed = time.time() - start_time
            stats["inference_times"].append(elapsed)

            # 4. ê²°ê³¼ ë¶„ì„ ë° Best-of-N ì„ ì • (í•µì‹¬)
            
            # (B * N_starts, 1) -> (B, N_starts)
            flat_rewards = result["reward"].view(current_batch_size, num_starts)
            flat_bom = result["bom_cost"].view(current_batch_size, num_starts)
            flat_sleep = result["sleep_cost"].view(current_batch_size, num_starts)

            # ê° ë¬¸ì œ(Instance)ë³„ë¡œ ê°€ì¥ ë†’ì€ ë³´ìƒì„ ë°›ì€ Trajectory ì„ íƒ
            best_rewards, best_indices = flat_rewards.max(dim=1) # (B,)
            
            # Best ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” Cost ì¶”ì¶œ
            # gatherë¥¼ ìœ„í•´ ì°¨ì› ë§ì¶¤: (B, 1)
            best_indices = best_indices.unsqueeze(1)
            best_bom = flat_bom.gather(1, best_indices).squeeze(1)
            best_sleep = flat_sleep.gather(1, best_indices).squeeze(1)

            # 5. í†µê³„ ì§‘ê³„
            for b in range(current_batch_size):
                r = best_rewards[b].item()
                bom = best_bom[b].item()
                sleep = best_sleep[b].item()
                
                stats["total_instances"] += 1
                stats["optimal_rewards"].append(r)
                
                # ì„±ê³µ ê¸°ì¤€: ì‹¤íŒ¨ í˜ë„í‹°ë³´ë‹¤ ë³´ìƒì´ ì»¤ì•¼ í•¨
                # (í™˜ê²½ ì„¤ì •ì˜ FAILURE_PENALTY = -20000.0)
                is_feasible = (r > FAILURE_PENALTY * 0.5) # ì—¬ìœ  ìˆê²Œ ì ˆë°˜ ì´ìƒì´ë©´ ì„±ê³µ ê°„ì£¼
                
                if is_feasible:
                    stats["feasible_instances"] += 1
                    stats["bom_costs"].append(bom)
                    stats["sleep_costs"].append(sleep)
            
            stats["avg_starts"] += num_starts

        # 6. ìµœì¢… ìš”ì•½
        stats["avg_starts"] /= num_batches
        feasibility_rate = (stats["feasible_instances"] / stats["total_instances"]) * 100
        avg_reward = np.mean(stats["optimal_rewards"])
        avg_bom = np.mean(stats["bom_costs"]) if stats["bom_costs"] else 0.0
        avg_sleep = np.mean(stats["sleep_costs"]) if stats["sleep_costs"] else 0.0
        avg_time = np.mean(stats["inference_times"])

        print("\n" + "="*50)
        print(f"ğŸ“Š Evaluation Summary (N={total_items})")
        print("="*50)
        print(f"âœ… Feasibility Rate : {feasibility_rate:.2f}% ({stats['feasible_instances']}/{stats['total_instances']})")
        print(f"ğŸ† Average Reward   : {avg_reward:.4f}")
        print(f"ğŸ’° Avg BOM Cost     : ${avg_bom:.4f} (Valid Only)")
        print(f"âš¡ Avg Sleep Penalty: {avg_sleep:.4f} (Valid Only)")
        print(f"â±ï¸ Avg Inference    : {avg_time:.4f} sec/batch")
        print(f"ğŸ”„ POMO Starts      : {stats['avg_starts']:.1f}")
        print("="*50)
        
        return stats