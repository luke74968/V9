# Copyright (c) 2025 Minuk Lee. All rights reserved.
# 
# This source code is proprietary and confidential.
# Unauthorized copying of this file, via any medium is strictly prohibited.
# 
# For licensing terms, see the LICENSE file.
# Contact: minuklee@snu.ac.kr
# 
import torch
import torch.nn as nn
import torch.nn.functional as F
import random # [ì¶”ê°€] Epsilon Mixingìš©
from torch.distributions import Categorical
from tensordict import TensorDict
from dataclasses import dataclass
from typing import Dict, List, Tuple

# --- í˜„ì¬ íŒ¨í‚¤ì§€(transformer_solver) ëª¨ë“ˆ ì„í¬íŠ¸ ---
from .definitions import (
    FEATURE_DIM, FEATURE_INDEX, SCALAR_PROMPT_FEATURE_DIM,
    NODE_TYPE_PADDING, NODE_TYPE_BATTERY, NODE_TYPE_LOAD, 
    NODE_TYPE_IC, NODE_TYPE_EMPTY
)
from .utils.common import batchify
from .solver_env import PocatEnv, BATTERY_NODE_IDX 


# ---
# ì„¹ì…˜ 1: í‘œì¤€ íŠ¸ëœìŠ¤í¬ë¨¸ ë¹Œë”© ë¸”ë¡ (íš¨ìœ¨ì„±)
# ---

class RMSNorm(nn.Module):
    """ Root Mean Square Layer Normalization """
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        output = self._norm(x.float()).type_as(x)
        return output * self.weight

class Normalization(nn.Module):
    """ ì •ê·œí™” ë ˆì´ì–´ ë˜í¼ (RMSNorm ë˜ëŠ” LayerNorm) """
    def __init__(self, embedding_dim, norm_type='rms', **kwargs):
        super().__init__()
        self.norm_type = norm_type
        if self.norm_type == 'rms':
            self.norm = RMSNorm(embedding_dim)
        elif self.norm_type == 'layer':
            self.norm = nn.LayerNorm(embedding_dim)
        else:
            raise NotImplementedError(f"Unknown norm_type: {norm_type}")

    def forward(self, x):
        return self.norm(x)

class ParallelGatedMLP(nn.Module):
    """ SwiGLU FFN (FeedForward) êµ¬í˜„ì²´ """
    def __init__(self, hidden_size: int, **kwargs):
        super().__init__()
        # LLAMA ì•„í‚¤í…ì²˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” FFN ì°¨ì› ê³„ì‚°
        inner_size = int(2 * hidden_size * 4 / 3)
        multiple_of = 256
        inner_size = multiple_of * ((inner_size + multiple_of - 1) // multiple_of)
        
        self.l1 = nn.Linear(hidden_size, inner_size, bias=False)
        self.l2 = nn.Linear(hidden_size, inner_size, bias=False)
        self.l3 = nn.Linear(inner_size, hidden_size, bias=False)
        self.act = F.silu

    def forward(self, z):
        z1 = self.l1(z)
        z2 = self.l2(z)
        return self.l3(self.act(z1) * z2)

def reshape_by_heads(qkv: torch.Tensor, head_num: int) -> torch.Tensor:
    """ (B, N, H*D) -> (B, H, N, D) """
    batch_s, n = qkv.size(0), qkv.size(1)
    q_reshaped = qkv.reshape(batch_s, n, head_num, -1)
    return q_reshaped.transpose(1, 2)

def multi_head_attention(q, k, v, attention_mask=None):
    """ 
    PyTorch 2.0+ Scaled Dot Product Attention (SDPA) ì ìš©
    (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë° ì†ë„ ìµœì í™” - FlashAttention ìë™ ì‚¬ìš©)
    """
    batch_s, head_num, n, key_dim = q.shape
    
    # SDPAë¥¼ ìœ„í•œ ë§ˆìŠ¤í¬ ì²˜ë¦¬
    # (PyTorch SDPAëŠ” Boolean ë§ˆìŠ¤í¬ ì§€ì›ì´ ë²„ì „ë§ˆë‹¤ ìƒì´í•˜ë¯€ë¡œ, 
    #  í™•ì‹¤í•˜ê²Œ -infë¥¼ ë”í•˜ëŠ” ë°©ì‹ì˜ Float ë§ˆìŠ¤í¬ë¡œ ë³€í™˜í•˜ì—¬ ì „ë‹¬)
    attn_mask = None
    if attention_mask is not None:
        if attention_mask.dim() == 3:
            attention_mask = attention_mask.unsqueeze(1) # (B, N, N) -> (B, 1, N, N)
        
        # True(ìœ íš¨) -> 0.0, False(ë§ˆìŠ¤í‚¹) -> -inf
        attn_mask = torch.zeros_like(attention_mask, dtype=q.dtype)
        attn_mask.masked_fill_(~attention_mask, -float('inf'))

    # PyTorch 2.0+ ìµœì í™” í•¨ìˆ˜ ì‚¬ìš©
    # (ë‚´ë¶€ì ìœ¼ë¡œ FlashAttention ë“±ì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ íšê¸°ì ìœ¼ë¡œ ì¤„ì„)
    out = F.scaled_dot_product_attention(
        q, k, v, 
        attn_mask=attn_mask
    )
    
    # 4. (B, H, N, D) -> (B, N, H*D)s
    out_transposed = out.transpose(1, 2)
    return out_transposed.contiguous().view(batch_s, n, head_num * key_dim)

class EncoderLayer(nn.Module):
    """ 
    í‘œì¤€ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ë ˆì´ì–´ (Post-Normalization)
    """
    def __init__(self, embedding_dim, head_num, qkv_dim, ffd='siglu', **model_params):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.head_num = head_num
        self.qkv_dim = qkv_dim
        
        self.Wq = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)
        self.Wk = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)
        self.Wv = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)
        self.multi_head_combine = nn.Linear(head_num * qkv_dim, embedding_dim)
        
        self.normalization1 = Normalization(embedding_dim, **model_params)
        
        if ffd == 'siglu':
            self.feed_forward = ParallelGatedMLP(hidden_size=embedding_dim, **model_params)
        else:
            raise NotImplementedError
            
        self.normalization2 = Normalization(embedding_dim, **model_params)

    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor = None) -> torch.Tensor:
        # 1. MHA (Post-Normalization)
        q = reshape_by_heads(self.Wq(x), self.head_num)
        k = reshape_by_heads(self.Wk(x), self.head_num)
        v = reshape_by_heads(self.Wv(x), self.head_num)
        
        mha_out = self.multi_head_combine(multi_head_attention(q, k, v, attention_mask=attention_mask))
        h = self.normalization1(x + mha_out) # Residual + Norm
        
        # 2. FFN (Post-Normalization)
        ffn_out = self.feed_forward(h)
        out = self.normalization2(h + ffn_out) # Residual + Norm
        return out

class PocatDecoderLayer(nn.Module):
    """
    Cross-Attentionê³¼ FFNìœ¼ë¡œ êµ¬ì„±ëœ ë””ì½”ë” ë ˆì´ì–´
    (Queryê°€ 1ê°œì´ë¯€ë¡œ Self-Attentionì€ ìƒëµí•˜ê³  Cross-Attentionì— ì§‘ì¤‘)
    """
    def __init__(self, embedding_dim, head_num, qkv_dim, **model_params):
        super().__init__()
        
        # 1. Cross-Attention (QueryëŠ” ì´ì „ ë ˆì´ì–´ ì¶œë ¥, Key/Valì€ ì¸ì½”ë” ì¶œë ¥)
        self.Wq = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)
        # (Wk, WvëŠ” ì¸ì½”ë” ìª½ì—ì„œ ë¯¸ë¦¬ ê³„ì‚°ëœ ìºì‹œë¥¼ ì¬ì‚¬ìš©í•˜ê±°ë‚˜, ì—¬ê¸°ì„œ ë³„ë„ ì •ì˜ ê°€ëŠ¥)
        # íš¨ìœ¨ì„±ì„ ìœ„í•´ ì—¬ê¸°ì„œëŠ” ì¸ì½”ë”ì˜ K, Vë¥¼ ê³µìœ (Sharing)í•˜ê±°ë‚˜ 
        # ë³„ë„ë¡œ íˆ¬ì˜(Projection)í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ë³„ë„ íˆ¬ì˜ì„ ê°€ì •í•©ë‹ˆë‹¤.
        self.Wk = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)
        self.Wv = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)
        
        self.multi_head_combine = nn.Linear(head_num * qkv_dim, embedding_dim)
        
        self.norm1 = Normalization(embedding_dim, **model_params)
        self.norm2 = Normalization(embedding_dim, **model_params)
        
        # 2. Feed Forward Network
        self.feed_forward = ParallelGatedMLP(hidden_size=embedding_dim, **model_params)
        
        self.head_num = head_num
        self.qkv_dim = qkv_dim

    def forward(self, x, cross_k, cross_v):
        """
        x: (B, 1, D) - í˜„ì¬ ë””ì½”ë”ì˜ Query ìƒíƒœ
        cross_k, cross_v: (B, H, N, D/H) - ë¯¸ë¦¬ ê³„ì‚°ëœ Key, Value
        """
        # --- Cross Attention ---
        # Query: í˜„ì¬ ë ˆì´ì–´ì˜ ì…ë ¥ x
        q = reshape_by_heads(self.Wq(x), self.head_num)
        
        # Key, Value: ì¸ìë¡œ ë°›ì€ ìºì‹œ ì‚¬ìš© (ì¬ê³„ì‚° X)
        # k = reshape_by_heads(self.Wk(encoder_out), self.head_num)
        # v = reshape_by_heads(self.Wv(encoder_out), self.head_num)
        
        mha_out = multi_head_attention(q, cross_k, cross_v)
        mha_out = self.multi_head_combine(mha_out)
        
        h = self.norm1(x + mha_out) # Residual + Norm
        
        # --- FFN ---
        ffn_out = self.feed_forward(h)
        out = self.norm2(h + ffn_out) # Residual + Norm
        
        return out
# ---
# ì„¹ì…˜ 2: ë””ì½”ë”© íš¨ìœ¨ì„ ìœ„í•œ ìºì‹œ
# ---

@dataclass
class PrecomputedCache:
    """
    ë””ì½”ë”© ë£¨í”„ì—ì„œ ë°˜ë³µ ê³„ì‚°ì„ í”¼í•˜ê¸° ìœ„í•´
    ì¸ì½”ë”ì˜ Key, Value ê°’ì„ ì €ì¥í•˜ëŠ” ìºì‹œ ê°ì²´ì…ë‹ˆë‹¤.
    """
    node_embeddings: torch.Tensor
    #glimpse_key: torch.Tensor
    #glimpse_val: torch.Tensor
    logit_key_connect: torch.Tensor # 'Connect' í¬ì¸í„°ìš© Key
    logit_key_spawn: torch.Tensor   # 'Spawn' í¬ì¸í„°ìš© Key
    # [ì¶”ê°€] ë””ì½”ë” ë ˆì´ì–´ë³„ Cross-Attention Key/Value ìºì‹œ
    decoder_layer_kvs: List[Tuple[torch.Tensor, torch.Tensor]] = None

    def batchify(self, num_starts: int):
        """ POMO ìƒ˜í”Œë§ì„ ìœ„í•´ ìºì‹œë¥¼ N_starts ë°°ìˆ˜ë§Œí¼ ë³µì œí•©ë‹ˆë‹¤. """
        # kv ë¦¬ìŠ¤íŠ¸ í™•ì¥
        new_kvs = []
        if self.decoder_layer_kvs:
            for k, v in self.decoder_layer_kvs:
                # [ìˆ˜ì •] repeat_interleave ì‚¬ìš©
                new_kvs.append((k.repeat_interleave(num_starts, dim=0), v.repeat_interleave(num_starts, dim=0)))

        return PrecomputedCache(
            self.node_embeddings.repeat_interleave(num_starts, dim=0),
            #batchify(self.glimpse_key, num_starts),
            #batchify(self.glimpse_val, num_starts),
            self.logit_key_connect.repeat_interleave(num_starts, dim=0),
            self.logit_key_spawn.repeat_interleave(num_starts, dim=0),
            new_kvs # [ì¶”ê°€]
        )

# ---
# ì„¹ì…˜ 3: POCAT ëª¨ë¸ ì•„í‚¤í…ì²˜
# ---

class PocatPromptNet(nn.Module):
    """
    ìŠ¤ì¹¼ë¼/í–‰ë ¬ ì œì•½ì¡°ê±´ì„ ì„ë² ë”©í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ë„¤íŠ¸ì›Œí¬ (N_MAX ëŒ€ì‘ ìˆ˜ì •íŒ)
    """
    def __init__(self, embedding_dim: int, N_MAX: int, **kwargs):
        super().__init__()
        self.N_max = N_MAX
        
        # 1. ìŠ¤ì¹¼ë¼ í”¼ì²˜ ì²˜ë¦¬ (ê¸°ì¡´ ìœ ì§€)
        self.scalar_net = nn.Sequential(
            nn.Linear(SCALAR_PROMPT_FEATURE_DIM, embedding_dim // 2),
            nn.ReLU(),
            nn.Linear(embedding_dim // 2, embedding_dim // 2)
        )

        # 2. ë§¤íŠ¸ë¦­ìŠ¤ í”¼ì²˜ ì²˜ë¦¬ (êµ¬ì¡° ë³€ê²½)
        # N*Nì„ í¼ì¹˜ëŠ” ëŒ€ì‹ , í•œ ì¤„(Row, N)ì”© ì²˜ë¦¬í•˜ì—¬ ì°¨ì›ì„ ì¤„ì…ë‹ˆë‹¤.
        self.matrix_proj = nn.Linear(N_MAX, embedding_dim // 2) 

        # ìµœì¢… ê²°í•©ë¶€
        self.final_net = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim),
            nn.ReLU(),
            nn.Linear(embedding_dim, embedding_dim)
        )

    def forward(self, scalar_features, matrix_features):
        # scalar_features: (B, SCALAR_DIM)
        # matrix_features: (B, N, N)

        # 1. ìŠ¤ì¹¼ë¼ ì„ë² ë”©
        scalar_emb = self.scalar_net(scalar_features) # (B, D/2)

        # 2. ë§¤íŠ¸ë¦­ìŠ¤ ì„ë² ë”© (Row-wise Projection & Mean Pooling)
        # (B, N, N) -> float ë³€í™˜ -> Linear -> (B, N, D/2)
        mat_feat = self.matrix_proj(matrix_features.float()) 
        
        # (B, N, D/2) -> í‰ê·  -> (B, D/2)
        matrix_emb = mat_feat.mean(dim=1) 

        # 3. ê²°í•©
        combined = torch.cat([scalar_emb, matrix_emb], dim=-1) # (B, D)
        
        # [ìˆ˜ì •] (B, D) -> (B, 1, D)ë¡œ ì°¨ì›ì„ ë§ì¶°ì¤ë‹ˆë‹¤.
        return self.final_net(combined).unsqueeze(1)


class PocatEncoder(nn.Module):
    """
    Pocat ì¸ì½”ë” (ë“€ì–¼ ì–´í…ì…˜ ë° ë‹¤ì¤‘ ì„ë² ë”© ì£¼ì…).
    
    1. ë…¸ë“œ íƒ€ì…(5ì¢…)ë³„ë¡œ ê¸°ë³¸ ì„ë² ë”© ì ìš©
    2. ë…¸ë“œ ì†ì„±/ìƒíƒœ(4ì¢…)ë³„ë¡œ ì¶”ê°€ ì„ë² ë”© ì£¼ì…
    3. ë“€ì–¼ ì–´í…ì…˜(Sparse/Global) í†µê³¼

    [ìˆ˜ì • ì‚¬í•­] 
    - On-the-fly Log Normalization ì ìš©
    - FEATURE_INDEX ì°¸ì¡° ì‹œ ì •ìˆ˜/íŠœí”Œ íƒ€ì… ì²´í¬í•˜ì—¬ ì—ëŸ¬ ìˆ˜ì •
    """
    def __init__(self, embedding_dim: int, encoder_layer_num: int, **model_params):
        super().__init__()
        
        # 1. ë…¸ë“œ "íƒ€ì…" (5ì¢…) ì„ë² ë”©
        self.embedding_padding = nn.Linear(FEATURE_DIM, embedding_dim)
        self.embedding_battery = nn.Linear(FEATURE_DIM, embedding_dim)
        self.embedding_load = nn.Linear(FEATURE_DIM, embedding_dim)
        self.embedding_ic = nn.Linear(FEATURE_DIM, embedding_dim)
        self.embedding_empty = nn.Linear(FEATURE_DIM, embedding_dim)
        
        # 2. ë…¸ë“œ "ì†ì„±/ìƒíƒœ" (4ì¢…) ì„ë² ë”© (0 ë˜ëŠ” 1 ê°’ì„ ì¸ë±ìŠ¤ë¡œ ì‚¬ìš©)
        self.embedding_is_active = nn.Embedding(2, embedding_dim)
        self.embedding_is_template = nn.Embedding(2, embedding_dim)
        self.embedding_can_spawn_into = nn.Embedding(2, embedding_dim)
        self.embedding_rail_type = nn.Embedding(3, embedding_dim) # 0:N/A, 1:Supp, 2:Path

        # 3. ë“€ì–¼ ì–´í…ì…˜(CaDA) ë ˆì´ì–´
        self.sparse_layers = nn.ModuleList([
            EncoderLayer(embedding_dim=embedding_dim, **model_params) 
            for _ in range(encoder_layer_num)
        ])
        self.global_layers = nn.ModuleList([
            EncoderLayer(embedding_dim=embedding_dim, **model_params) 
            for _ in range(encoder_layer_num)
        ])
        self.sparse_fusion = nn.ModuleList([
            nn.Linear(embedding_dim, embedding_dim) 
            for _ in range(encoder_layer_num)
        ])
        self.global_fusion = nn.ModuleList([
            nn.Linear(embedding_dim, embedding_dim) 
            for _ in range(encoder_layer_num - 1)
        ])

    def forward(self, td: TensorDict, prompt_embedding: torch.Tensor) -> torch.Tensor:
        # 1. ì›ë³¸ ë…¸ë“œ í”¼ì²˜ ê°€ì ¸ì˜¤ê¸° (ë§ˆìŠ¤í‚¹ ë“± ë¬¼ë¦¬ ê³„ì‚°ìš© ì›ë³¸ ë³´ì¡´)
        raw_node_features = td['nodes'] # (B, N_MAX, FEATURE_DIM)
        
        # 2. [í•µì‹¬] ëª¨ë¸ ì…ë ¥ìš©ìœ¼ë¡œ ë³µì œ í›„ On-the-fly Log Normalization ì ìš©
        # clone()ì„ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ td['nodes']ê°€ ìˆ˜ì •ë˜ì§€ ì•Šë„ë¡ í•¨
        nodes_input = raw_node_features.clone()
        
        # ---------------------------------------------------------------------
        # ì „ëµ A: ê°’ì´ ë„ˆë¬´ í° ë³€ìˆ˜ (Large Scale) -> Log1p ì ìš©
        # (ì €í•­: ìˆ˜ì‹­ë§Œ~ìˆ˜ì¡°, ì—´ì €í•­: ìˆ˜ë°±, ë¹„ìš©: ìˆ˜ì‹­)
        # ---------------------------------------------------------------------
        large_val_keys = ["min_fb_res", "theta_ja", "cost"]
        
        for key in large_val_keys:
            if key in FEATURE_INDEX:
                # [ìˆ˜ì •] definitions.pyì—ì„œ ìŠ¤ì¹¼ë¼ í”¼ì²˜ëŠ” ì •ìˆ˜í˜•ì´ë¯€ë¡œ ë°”ë¡œ ì‚¬ìš©
                idx = FEATURE_INDEX[key]
                if isinstance(idx, tuple): idx = idx[0] # í˜¹ì‹œ ëª¨ë¥¼ íŠœí”Œì¼ ê²½ìš° ëŒ€ë¹„
                
                nodes_input[..., idx] = torch.log1p(torch.relu(nodes_input[..., idx]))

        # ---------------------------------------------------------------------
        # ì „ëµ B: ê°’ì´ ë„ˆë¬´ ì‘ì€ ë³€ìˆ˜ (Tiny Scale) -> uA ë³€í™˜(x 1e6) í›„ Log1p ì ìš©
        # (ì „ë¥˜: ìˆ˜ mA ~ ìˆ˜ uA ìˆ˜ì¤€ -> ëª¨ë¸ì´ 0ìœ¼ë¡œ ì¸ì‹í•˜ëŠ” ë¬¸ì œ ë°©ì§€)
        # ---------------------------------------------------------------------
        small_val_keys = [
            "current_active",      # Load Active Current
            "current_sleep",       # Load Sleep Current
            "op_current",          # Operating Current
            "quiescent_current",   # Quiescent Current
            "shutdown_current",    # Shutdown Current
            "not_switching_current"# Buck Non-switching Current
        ]
        
        for key in small_val_keys:
            if key in FEATURE_INDEX:
                idx = FEATURE_INDEX[key]
                if isinstance(idx, tuple): idx = idx[0]

                # x 1,000,000 (uA ë‹¨ìœ„ë¡œ ë³€í™˜)
                # ì˜ˆ: 1uA (1e-6) -> 1.0 -> ln(2) = 0.69
                val = torch.relu(nodes_input[..., idx]) * 1e6 
                nodes_input[..., idx] = torch.log1p(val)

        # ---------------------------------------------------------------------
        # ì „ëµ C: ì¼ë°˜ì ì¸ ë³€ìˆ˜ (Normal Scale) -> ê·¸ëŒ€ë¡œ í†µê³¼
        # (Vin, Vout, I_limit, Efficiency ë“±ì€ 0~50 ë²”ìœ„ ë‚´ë¼ ë³€í™˜ ë¶ˆí•„ìš”)
        # ---------------------------------------------------------------------
        
        batch_size, num_nodes, _ = nodes_input.shape # num_nodes = N_MAX
        embedding_dim = self.embedding_battery.out_features
        
        # AMP ì‹¤í–‰ ì‹œ ì„ë² ë”© ì¶œë ¥ì´ fp16/bf16ìœ¼ë¡œ ë‹¤ìš´ìºìŠ¤íŒ…ë˜ë¯€ë¡œ,
        # node_embeddingsì˜ dtypeì„ í˜„ì¬ autocast dtypeì— ë§ì¶° ìƒì„±í•œë‹¤.
        if torch.is_autocast_enabled():
            try:
                embedding_dtype = torch.get_autocast_gpu_dtype()
            except Exception:
                # PyTorch < 2.0ì—ëŠ” get_autocast_gpu_dtypeê°€ ì—†ìœ¼ë¯€ë¡œ fp16ìœ¼ë¡œ ê³ ì •
                embedding_dtype = torch.float16
        else:
            embedding_dtype = nodes_input.dtype
        node_embeddings = torch.zeros(
            batch_size, num_nodes, embedding_dim,
            device=nodes_input.device, dtype=embedding_dtype
        )

        # --- 1. íƒ€ì…ë³„ ê¸°ë³¸ ì„ë² ë”© ì ìš© (ë³€í™˜ëœ nodes_input ì‚¬ìš©) ---
        # node_typeì€ One-hot ì¸ì½”ë”©ëœ ë²”ìœ„ë¥¼ ê°€ì§€ë¯€ë¡œ íŠœí”Œ ì¸ë±ì‹± ìœ ì§€
        node_type_indices = nodes_input[..., FEATURE_INDEX["node_type"][0]:FEATURE_INDEX["node_type"][1]].argmax(dim=-1)
        
        masks = {
            NODE_TYPE_PADDING: (node_type_indices == NODE_TYPE_PADDING),
            NODE_TYPE_BATTERY: (node_type_indices == NODE_TYPE_BATTERY),
            NODE_TYPE_LOAD: (node_type_indices == NODE_TYPE_LOAD),
            NODE_TYPE_IC: (node_type_indices == NODE_TYPE_IC),
            NODE_TYPE_EMPTY: (node_type_indices == NODE_TYPE_EMPTY),
        }
        
        # ê° íƒ€ì…ë³„ ì„ë² ë”©ì„ node_embeddings dtypeìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í• ë‹¹í•œë‹¤.
        if masks[NODE_TYPE_PADDING].any():
            emb = self.embedding_padding(nodes_input[masks[NODE_TYPE_PADDING]])
            node_embeddings[masks[NODE_TYPE_PADDING]] = emb.to(node_embeddings.dtype)
        if masks[NODE_TYPE_BATTERY].any():
            emb = self.embedding_battery(nodes_input[masks[NODE_TYPE_BATTERY]])
            node_embeddings[masks[NODE_TYPE_BATTERY]] = emb.to(node_embeddings.dtype)
        if masks[NODE_TYPE_LOAD].any():
            emb = self.embedding_load(nodes_input[masks[NODE_TYPE_LOAD]])
            node_embeddings[masks[NODE_TYPE_LOAD]] = emb.to(node_embeddings.dtype)
        if masks[NODE_TYPE_IC].any():
            emb = self.embedding_ic(nodes_input[masks[NODE_TYPE_IC]])
            node_embeddings[masks[NODE_TYPE_IC]] = emb.to(node_embeddings.dtype)
        if masks[NODE_TYPE_EMPTY].any():
            emb = self.embedding_empty(nodes_input[masks[NODE_TYPE_EMPTY]])
            node_embeddings[masks[NODE_TYPE_EMPTY]] = emb.to(node_embeddings.dtype)

        # --- 2. ì†ì„±/ìƒíƒœ ì„ë² ë”© ì£¼ì… (Injection) ---
        # (ì¸ë±ìŠ¤ ê°’ë“¤ì€ ì •ìˆ˜í˜•ì´ë¯€ë¡œ ë¡œê·¸ ë³€í™˜ì˜ ì˜í–¥ì„ ë°›ì§€ ì•Šì§€ë§Œ, ì¼ê´€ì„±ì„ ìœ„í•´ nodes_inputì—ì„œ ê°€ì ¸ì˜´)
        active_ids = nodes_input[..., FEATURE_INDEX["is_active"]].long()
        template_ids = nodes_input[..., FEATURE_INDEX["is_template"]].long()
        spawn_ids = nodes_input[..., FEATURE_INDEX["can_spawn_into"]].long()
        rail_ids = nodes_input[..., FEATURE_INDEX["independent_rail_type"]].round().long().clamp(0, 2)
        
        # ì†ì„±/ìƒíƒœ ì„ë² ë”©ë„ dtypeì„ ë§ì¶° ë”í•œë‹¤.
        node_embeddings.add_(self.embedding_is_active(active_ids).to(node_embeddings.dtype))
        node_embeddings.add_(self.embedding_is_template(template_ids).to(node_embeddings.dtype))
        node_embeddings.add_(self.embedding_can_spawn_into(spawn_ids).to(node_embeddings.dtype))
        node_embeddings.add_(self.embedding_rail_type(rail_ids).to(node_embeddings.dtype))

        
        # --- 3. ë“€ì–¼ ì–´í…ì…˜ (CaDA) ì‹¤í–‰ ---
        connectivity_mask = td['connectivity_matrix'] # (B, N_MAX, N_MAX)
        attention_mask = td['attention_mask'] # (B, N_MAX, N_MAX)

        global_input = torch.cat((node_embeddings, prompt_embedding), dim=1)
        
        global_attention_mask = torch.zeros(
            batch_size, num_nodes + 1, num_nodes + 1, 
            dtype=torch.bool, device=node_embeddings.device
        )
        global_attention_mask[:, :num_nodes, :num_nodes] = attention_mask
        
        alive_mask_1d = (node_type_indices != NODE_TYPE_PADDING)
        global_attention_mask[:, num_nodes, :num_nodes] = alive_mask_1d
        global_attention_mask[:, :num_nodes, num_nodes] = alive_mask_1d
        global_attention_mask[:, num_nodes, num_nodes] = True
        
        sparse_out, global_out = node_embeddings, global_input
        for i in range(len(self.sparse_layers)):
            sparse_out = self.sparse_layers[i](sparse_out, attention_mask=connectivity_mask)
            global_out = self.global_layers[i](global_out, attention_mask=global_attention_mask)
            
            sparse_out = sparse_out + self.sparse_fusion[i](global_out[:, :num_nodes])
            if i < len(self.global_layers) - 1:
                global_nodes = global_out[:, :num_nodes] + self.global_fusion[i](sparse_out)
                global_out = torch.cat((global_nodes, global_out[:, num_nodes:]), dim=1)  
                
        return global_out[:, :num_nodes] # í”„ë¡¬í”„íŠ¸ ì„ë² ë”© ì œì™¸ (B, N_MAX, D)

class PocatDecoder(nn.Module):
    def __init__(self, embedding_dim, head_num, qkv_dim, N_MAX, **model_params):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.head_num = head_num
        self.qkv_dim = qkv_dim
        self.N_MAX = N_MAX
        
        # config.yamlì—ì„œ decoder_layer_numì„ ê°€ì ¸ì˜µë‹ˆë‹¤ (ê¸°ë³¸ê°’ 1)
        self.layer_num = model_params.get('decoder_layer_num', 1)

        # 1. ì´ˆê¸° ì»¨í…ìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±ìš© (ì…ë ¥ ì°¨ì› ë³€í™˜)
        # (embedding_dim + 3 features) -> embedding_dim
        self.input_projector = nn.Linear(embedding_dim + 3, embedding_dim)

        # 2. ë””ì½”ë” ë ˆì´ì–´ ìŠ¤íƒ (ModuleList)
        self.layers = nn.ModuleList([
            PocatDecoderLayer(embedding_dim, head_num, qkv_dim, **model_params)
            for _ in range(self.layer_num)
        ])
        
        # 3. í¬ì¸í„° ë„¤íŠ¸ì›Œí¬ìš© Key ìƒì„± (ì¸ì½”ë” ì„ë² ë”©ì„ ë³€í™˜)
        self.Wk_connect_logit = nn.Linear(embedding_dim, embedding_dim, bias=False)
        self.Wk_spawn_logit = nn.Linear(embedding_dim, embedding_dim, bias=False)

        # --- 4. 4-Heads (q_vecì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ) ---
        self.value_head = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim // 2),
            nn.ReLU(),
            nn.Linear(embedding_dim // 2, 1)
        )
        self.type_head = nn.Linear(embedding_dim, 2)
        self.connect_head = nn.Linear(embedding_dim, embedding_dim)
        self.spawn_head = nn.Linear(embedding_dim, embedding_dim)

    def forward(self, td: TensorDict, cache: PrecomputedCache) -> Tuple[torch.Tensor, ...]:
        
        # 1. ì´ˆê¸° ì¿¼ë¦¬ ì…ë ¥ ì¤€ë¹„
        avg_current = td["nodes"][..., FEATURE_INDEX["current_out"]].clone().mean(dim=1, keepdim=True)
        unconnected_ratio = td["unconnected_loads_mask"].clone().float().mean(dim=1, keepdim=True)
        step_ratio = td["step_count"].clone().float() / (2 * self.N_MAX)
        state_features = torch.cat([avg_current, unconnected_ratio, step_ratio], dim=1)

        head_idx = td["trajectory_head"].detach().squeeze(-1).clone()
        batch_indices = torch.arange(td.batch_size[0], device=head_idx.device)
        head_emb = cache.node_embeddings[batch_indices, head_idx]
        
        # (B, D+3) -> (B, 1, D)
        query_input = torch.cat([head_emb, state_features], dim=1).unsqueeze(1)
        
        # ì´ˆê¸° q_vec (Projection)
        q_vec = self.input_projector(query_input)

        # 2. ë””ì½”ë” ë ˆì´ì–´ ìˆœì°¨ í†µê³¼ (Stacking)
        # q_vecì´ ê° ë ˆì´ì–´ë¥¼ ê±°ì¹˜ë©° ì ì  ë” ì •êµí•œ Context Vectorê°€ ë©ë‹ˆë‹¤.
        # encoder_out = cache.node_embeddings # (B, N, D) <-- ì‚­ì œ (ìºì‹œ ì‚¬ìš©)
        
        for i, layer in enumerate(self.layers):
            k_cache, v_cache = cache.decoder_layer_kvs[i]
            q_vec = layer(q_vec, k_cache, v_cache)

        # --- 3. ìµœì¢… ê²°ì • (Heads) ---
        value = self.value_head(q_vec).squeeze(-1)
        logits_action_type = self.type_head(q_vec).squeeze(1)
        
        query_connect = self.connect_head(q_vec) 
        logits_connect_target = torch.matmul(
            query_connect, cache.logit_key_connect
        ).squeeze(1) / (self.embedding_dim ** 0.5)
        
        query_spawn = self.spawn_head(q_vec) 
        logits_spawn_template = torch.matmul(
            query_spawn, cache.logit_key_spawn
        ).squeeze(1) / (self.embedding_dim ** 0.5)

        return logits_action_type, logits_connect_target, logits_spawn_template, value

class PocatModel(nn.Module):
    """
    Pocat V7 (Padding + Lazy Spawn) ë©”ì¸ ëª¨ë¸
    """
    
    def __init__(self, **model_params):
        super().__init__()
        self.logit_clipping = model_params.get('logit_clipping', 10)
        
        # config.yamlì—ì„œ N_MAX ì£¼ì…
        self.N_MAX = model_params['N_MAX']
        # model_paramsì—ì„œ N_MAXë¥¼ popí•˜ì—¬ ì¤‘ë³µ ì „ë‹¬ ë°©ì§€
        # (PocatPromptNetê³¼ PocatDecoderëŠ” N_MAXë¥¼ ëª…ì‹œì  ì¸ìë¡œ ë°›ìŒ)s
        n_max_value = model_params.pop('N_MAX')
        self.prompt_net = PocatPromptNet(N_MAX=n_max_value, **model_params)
        self.encoder = PocatEncoder(**model_params)
        self.decoder = PocatDecoder(N_MAX=n_max_value, **model_params)

    def _get_masked_probs(self, logits, mask):
        """ ë¡œì§“ê³¼ ë§ˆìŠ¤í¬ë¥¼ ë°›ì•„ ì •ê·œí™”ëœ í™•ë¥  ë¶„í¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. """
        scores = self.logit_clipping * torch.tanh(logits)
        scores.masked_fill_(~mask, -float('inf'))
        
        # [ìˆ˜ì •] NaN ë° In-place ì˜¤ë¥˜ ë°©ì§€ (Out-of-place)
        # ì „ë¶€ -infì¸ ê²½ìš°(ë§‰ë‹¤ë¥¸ ê¸¸) -> softmaxê°€ NaN ë°˜í™˜ ë°©ì§€
        if (scores == -float('inf')).all(dim=-1).any():
             # ë§ˆìŠ¤í‚¹ëœ ê³³ì€ ê·¸ëŒ€ë¡œ ë‘ê³ , ì „ë¶€ ë§ˆìŠ¤í‚¹ëœ í–‰ë§Œ 0ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ softmax ê³„ì‚°
             safe_scores = scores.clone()
             safe_scores[safe_scores == -float('inf')] = -1e9 # ë§¤ìš° ì‘ì€ ìˆ˜ë¡œ ëŒ€ì²´
             probs = F.softmax(safe_scores, dim=-1)
        else:
             probs = F.softmax(scores, dim=-1)

        return probs  

    def _sample_action(self, logits, mask, decode_type, temperature=1.0): # [ì¶”ê°€] temperature
        """ 
        ë¡œì§“ê³¼ ë§ˆìŠ¤í¬ë¥¼ ë°›ì•„ ì•¡ì…˜(idx)ê³¼ ë¡œê·¸ í™•ë¥ (log_prob)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        (ë§‰ë‹¤ë¥¸ ê¸¸ ë°©ì§€ ë¡œì§ í¬í•¨)
        """
        scores = self.logit_clipping * torch.tanh(logits)
        scores.masked_fill_(~mask, -float('inf'))

        # [ì¶”ê°€] Temperature Scaling (í™•ë¥  ë¶„í¬ë¥¼ í‰í‰í•˜ê²Œ ë§Œë“¦)
        # ê°’ì´ í´ìˆ˜ë¡(>1.0) ë¬´ì‘ìœ„ì„±ì´ ê°•í•´ì§
        scores = scores / temperature

        # ëª¨ë“  ì•¡ì…˜ì´ ë§ˆìŠ¤í‚¹ëœ 'ë§‰ë‹¤ë¥¸ ê¸¸' ìƒíƒœ ë°©ì§€
        # (ëª¨ë“  ê°’ì´ -infì´ë©´ maxë„ -inf)
        max_scores, _ = scores.max(dim=-1)
        is_stuck = (max_scores == -float('inf'))
        
        # [ìˆ˜ì •] In-place ì—°ì‚° ë°©ì§€: scores[is_stuck, 0] = 0.0 ëŒ€ì‹  torch.where ì‚¬ìš©
        if is_stuck.any():
            # ë§‰ë‹¤ë¥¸ ê¸¸ì¸ ê²½ìš° 0ë²ˆ ì¸ë±ìŠ¤ì— 0.0ì„ í• ë‹¹í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ìœ ì§€
            # (In-place í• ë‹¹ ëŒ€ì‹  ë§ˆìŠ¤í‚¹ìœ¼ë¡œ ì²˜ë¦¬)
            fallback_scores = torch.zeros_like(scores)
            fallback_scores.fill_(-float('inf'))
            fallback_scores[:, 0] = 0.0
            
            scores = torch.where(is_stuck.unsqueeze(-1), fallback_scores, scores)
        
        log_prob = F.log_softmax(scores, dim=-1)
        probs = log_prob.exp()
        # [ìˆ˜ì •] NaN ì•ˆì „ì¥ì¹˜ (In-place ì—°ì‚° ì™„ì „ ë°°ì œ)
        if torch.isnan(probs).any():
            # 1. NaNì„ 0ìœ¼ë¡œ ëŒ€ì²´ (Out-of-place)
            clean_probs = torch.where(torch.isnan(probs), torch.zeros_like(probs), probs)
            
            # 2. í•©ì´ 0ì¸ í–‰(ì „ë¶€ NaN/Masked) ì²˜ë¦¬
            sum_probs = clean_probs.sum(dim=-1, keepdim=True)
            # í•©ì´ 0ì´ë©´ ê· ë“± ë¶„í¬(ë˜ëŠ” 0ë²ˆ ëª°ë¹µ)ë¡œ ëŒ€ì²´
            fallback_probs = torch.zeros_like(clean_probs)
            fallback_probs[:, 0] = 1.0
            
            final_probs = torch.where(sum_probs == 0, fallback_probs, clean_probs)
            probs = final_probs # êµì²´
            
            # 3. Log Prob ì¬ê³„ì‚° (ê·¸ë˜í”„ ë‹¨ì ˆ ì—†ì´ ì•ˆì „í•˜ê²Œ)
            log_prob = torch.log(probs + 1e-10)
      
        # --- [ì¶”ê°€] ì—”íŠ¸ë¡œí”¼ ê³„ì‚° ---
        dist = Categorical(probs=probs)
        entropy = dist.entropy()
        # ---------------------------

        if decode_type == 'greedy':
            action = probs.argmax(dim=-1)
        else: # 'sampling'
            # -------------------------------------------------------
            # ğŸ›¡ï¸ ì „ëµ 2: Epsilon Mixing (Policy + Uniform Sampling)
            #    - í•™ìŠµ ì¤‘ì¼ ë•Œ 10% í™•ë¥ ë¡œ Policy ë¬´ì‹œí•˜ê³  ë¬´ì‘ìœ„ íƒìƒ‰
            # -------------------------------------------------------
            if self.training and random.random() < 0.1:
                # [ìˆ˜ì •] ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì§€ì›í•˜ëŠ” Uniform Sampling
                # (ëœë¤ ë…¸ì´ì¦ˆë¥¼ ë”í•´ì„œ argmaxë¥¼ í•˜ë©´ ìœ íš¨í•œ ì•¡ì…˜ ì¤‘ ëœë¤ ì„ íƒê³¼ ë™ì¼í•¨)
                random_noise = torch.rand_like(logits)
                random_noise.masked_fill_(~mask, -float('inf')) # ë¬´íš¨ ì•¡ì…˜ ì œì™¸
                action = random_noise.argmax(dim=-1) # (B,) í¬ê¸°ì˜ ëœë¤ ì•¡ì…˜ ë°˜í™˜
            else:
                # ê¸°ì¡´ Policy Sampling (90%)
                action = Categorical(probs=probs).sample()
            # -------------------------------------------------------
            
        # ì„ íƒëœ ì•¡ì…˜ì˜ ë¡œê·¸ í™•ë¥  ë°˜í™˜
        return action, log_prob.gather(1, action.unsqueeze(-1)).squeeze(-1), entropy

    def _combine_log_probs(self, 
                           log_prob_type, action_type, 
                           log_prob_connect, log_prob_spawn):
        """
        Parameterized Actionì˜ ë¡œê·¸ í™•ë¥ ì„ ê²°í•©í•©ë‹ˆë‹¤.
        logÏ€(a|s) = logÏ€(type|s) + logÏ€(arg|type,s)
        """
        # 'Connect' (0)ë¥¼ ì„ íƒí•œ ê²½ìš°ì˜ ë¡œê·¸ í™•ë¥ 
        log_prob_if_connect = log_prob_type + log_prob_connect
        # 'Spawn' (1)ì„ ì„ íƒí•œ ê²½ìš°ì˜ ë¡œê·¸ í™•ë¥ 
        log_prob_if_spawn = log_prob_type + log_prob_spawn
        
        # (B,)
        final_log_prob = torch.where(
            action_type == 0,       # 'Connect'ë¥¼ ì„ íƒí–ˆìœ¼ë©´
            log_prob_if_connect,    # ì´ í™•ë¥ ì„ ì‚¬ìš©
            log_prob_if_spawn       # ì•„ë‹ˆë©´ (Spawn) ì´ í™•ë¥ ì„ ì‚¬ìš©
        )
        return final_log_prob

    def forward(self, 
                td: TensorDict, 
                env: PocatEnv, # (solver_env.pyì˜ í™˜ê²½ ê°ì²´)
                decode_type: str = 'greedy', 
                pbar: object = None,
                status_msg: str = "", 
                log_fn=None, log_idx: int = 0, 
                log_mode: str = 'progress',
                return_final_td: bool = False,   # ğŸ‘ˆ ì´ ì¤„ ì¶”ê°€
                ) -> Dict[str, torch.Tensor]:
        
        base_desc = pbar.desc.split(' | ')[0] if pbar else ""
        if pbar: pbar.set_description(f"{base_desc} | {status_msg} | â–¶ Encoding")
        
        # --- 1. ì¸ì½”ë”© ë° ìºì‹œ ìƒì„± ---
        prompt_embedding = self.prompt_net(td["scalar_prompt_features"], td["matrix_prompt_features"])
        encoded_nodes = self.encoder(td, prompt_embedding) # (B, N_MAX, D)
        
        # ë””ì½”ë”ê°€ ì‚¬ìš©í•  Key/Value ì‚¬ì „ ê³„ì‚°
        #glimpse_key = reshape_by_heads(self.decoder.Wk_glimpse(encoded_nodes), self.decoder.head_num)
        #glimpse_val = reshape_by_heads(self.decoder.Wv_glimpse(encoded_nodes), self.decoder.head_num)
        
        # í¬ì¸í„° í—¤ë“œë³„ Key ìƒì„±
        logit_key_connect = self.decoder.Wk_connect_logit(encoded_nodes).transpose(1, 2)
        logit_key_spawn = self.decoder.Wk_spawn_logit(encoded_nodes).transpose(1, 2)

        # [ì¶”ê°€] ë””ì½”ë” ë ˆì´ì–´ìš© K, V ë¯¸ë¦¬ ê³„ì‚° (Pre-computation)
        # ë£¨í”„ ë°–ì—ì„œ í•œ ë²ˆë§Œ ê³„ì‚°í•˜ë¯€ë¡œ ë©”ëª¨ë¦¬ì™€ ì—°ì‚°ëŸ‰ì´ íšê¸°ì ìœ¼ë¡œ ì¤„ì–´ë“­ë‹ˆë‹¤.
        decoder_layer_kvs = []
        for layer in self.decoder.layers:
            # (B, N, D) -> (B, H, N, D/H)
            k = reshape_by_heads(layer.Wk(encoded_nodes), layer.head_num)
            v = reshape_by_heads(layer.Wv(encoded_nodes), layer.head_num)
            decoder_layer_kvs.append((k, v))

        cache = PrecomputedCache(
            node_embeddings=encoded_nodes,
            #glimpse_key=glimpse_key,
            #glimpse_val=glimpse_val,
            logit_key_connect=logit_key_connect,
            logit_key_spawn=logit_key_spawn,
            decoder_layer_kvs=decoder_layer_kvs # [ì¶”ê°€]
        )
        
        # --- 2. POMO (Multi-Start) ì¤€ë¹„ ---
        num_starts, start_nodes_idx = env.select_start_nodes(td)
        if num_starts == 0:
             # (B, 1) í˜•íƒœì˜ 0ì  ë¦¬ì›Œë“œ ë°˜í™˜
            zero_reward = torch.zeros(td.batch_size[0], 1, device=td.device)
            return {"reward": zero_reward} # (POMO ì‹œì‘ ë¶ˆê°€)

        batch_size = td.batch_size[0]
        
        # (B) -> (B * num_starts)
        # [ìˆ˜ì •] utils.batchify ëŒ€ì‹  tensordict ë‚´ì¥ ë©”ì„œë“œ ì‚¬ìš©
        # (POMO: ê° ìƒ˜í”Œì„ num_startsë§Œí¼ ë³µì œ)
        td = td.repeat_interleave(num_starts, dim=0)

        cache = cache.batchify(num_starts) # ìºì‹œë„ í™•ì¥

        # POMO ì‹œì‘: ì²« ì•¡ì…˜(Load ì„ íƒ)ì„ í™˜ê²½ì— ê°•ì œ ì ìš©
        first_action_tensor = start_nodes_idx.repeat(batch_size).unsqueeze(-1)
        
        # (POMOì˜ ì²« ìŠ¤í…ì€ env._resetì—ì„œ ì²˜ë¦¬ë˜ë„ë¡ solver_env.pyì—ì„œ êµ¬í˜„ í•„ìš”)
        # (ì—¬ê¸°ì„œëŠ” tdê°€ ì´ë¯¸ ì²« Loadê°€ Headë¡œ ì„¤ì •ëœ ìƒíƒœë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.)
        
        # --- 3. ë””ì½”ë”© ë£¨í”„ ---
        log_probs: List[torch.Tensor] = []
        actions: List[Dict[str, torch.Tensor]] = []
        rewards: List[torch.Tensor] = []
        entropies: List[torch.Tensor] = [] # [ì¶”ê°€] ì—”íŠ¸ë¡œí”¼ ì €ì¥ìš©
        first_value: torch.Tensor = None
        
        decoding_step = 0
        while not td["done"].all():
            decoding_step += 1
            if pbar and log_mode == 'progress':
                # [ìˆ˜ì •] 0ë²ˆ ìƒ˜í”Œì˜ ì‹¤ì œ Load ê°œìˆ˜ ê³„ì‚° (ëœë¤ ë°°ì¹˜ ëŒ€ì‘)
                node_types = td["nodes"][0, :, FEATURE_INDEX["node_type"][0]:FEATURE_INDEX["node_type"][1]].argmax(-1)
                sample_num_loads = (node_types == NODE_TYPE_LOAD).sum().item()
                
                # í˜„ì¬ ë‚¨ì€ ì—°ê²° ì•ˆ ëœ Load ìˆ˜
                unconnected = td['unconnected_loads_mask'][0].sum().item() 
                connected = sample_num_loads - unconnected
                
                pbar.set_description(f"{base_desc} | {status_msg} | Loads {connected}/{sample_num_loads}")

            # 1. ë””ì½”ë” í˜¸ì¶œ (4ê°œ í…ì„œ ë°˜í™˜)
            logits_type, logits_connect, logits_spawn, value = self.decoder(td, cache)
            
            # A2Cë¥¼ ìœ„í•´ ì²« ìŠ¤í…ì˜ Value(ê°€ì¹˜) ì €ì¥
            if decoding_step == 1:
                first_value = value.squeeze(-1) # (B * N_loads, 1) -> (B * N_loads)
            
            # 2. í™˜ê²½ì—ì„œ 3ì¢… ë§ˆìŠ¤í¬ ê°€ì ¸ì˜¤ê¸°
            # (solver_env.pyê°€ ë°˜í™˜í•  ë§ˆìŠ¤í¬ ë”•ì…”ë„ˆë¦¬)
            with torch.no_grad():
                masks: Dict[str, torch.Tensor] = env.get_action_mask(td)
            
            # [ì¶”ê°€] Temperature ìŠ¤ì¼€ì¤„ë§ (í•™ìŠµ ëª¨ë“œì¼ ë•Œë§Œ ì ìš©)
            # í•™ìŠµ ì´ˆë°˜ì—ëŠ” 5.0 ë“±ìœ¼ë¡œ ë†’ê²Œ ì„¤ì •í•˜ì—¬ ê°•ì œ íƒìƒ‰ ìœ ë„ í•„ìš”
            temp = 1.0 
            if self.training: # model.train() ìƒíƒœì¼ ë•Œ
                 # ì˜ˆ: ë¡œê·¸ ë“±ì„ í†µí•´ ì™¸ë¶€ì—ì„œ ì œì–´í•˜ê±°ë‚˜, ì¼ë‹¨ ìƒìˆ˜ë¡œ í…ŒìŠ¤íŠ¸
                 temp = 2.0


            # 3. 3ê°œ í—¤ë“œì—ì„œ ê°ê° ìƒ˜í”Œë§
            action_type, log_prob_type, ent_type = self._sample_action(
                logits_type, masks["mask_type"], decode_type, temperature=temp
            )
            action_connect, log_prob_connect, ent_connect = self._sample_action(
                logits_connect, masks["mask_connect"], decode_type, temperature=temp
            )
            action_spawn, log_prob_spawn, ent_spawn = self._sample_action(
                logits_spawn, masks["mask_spawn"], decode_type, temperature=temp
            )

            # [ì¶”ê°€] ìŠ¤í…ë³„ ì´ ì—”íŠ¸ë¡œí”¼ í•© ì‚° (Action Type + Argument)
            # Connectë¥¼ ê³¨ëìœ¼ë©´ Connect ì—”íŠ¸ë¡œí”¼, Spawnì´ë©´ Spawn ì—”íŠ¸ë¡œí”¼ ì‚¬ìš©
            step_entropy = ent_type + torch.where(action_type == 0, ent_connect, ent_spawn)
            entropies.append(step_entropy)

            # 4. Parameterized Action Log Prob ê²°í•©
            final_log_prob = self._combine_log_probs(
                log_prob_type, action_type, 
                log_prob_connect, log_prob_spawn
            )
            
            # 5. í™˜ê²½ì— ì „ë‹¬í•  ì•¡ì…˜ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            action_dict = {
                "action_type": action_type.unsqueeze(-1),
                "connect_target": action_connect.unsqueeze(-1),
                "spawn_template": action_spawn.unsqueeze(-1),
            }
            
            # [START]: 'detail' ëª¨ë“œ ì•¡ì…˜ ë¡œê¹… (ìˆ˜ì •ë¨)
            if log_fn and log_mode == 'detail':
                # (ì²« ë²ˆì§¸ ìƒ˜í”Œ(B=0) ê¸°ì¤€ìœ¼ë¡œ ë¡œê·¸ ì¶œë ¥)
                sample_idx = 0
                if sample_idx < td.batch_size[0]:
                    current_head = td["trajectory_head"][sample_idx].item()
                    
                    # --- 1. í™•ë¥  ë¶„í¬ ê³„ì‚° ---
                    # (ìœ„ì—ì„œ ì •ì˜í•œ _get_masked_probs ì‚¬ìš©)
                    probs_type = self._get_masked_probs(logits_type[sample_idx], masks["mask_type"][sample_idx])
                    probs_connect = self._get_masked_probs(logits_connect[sample_idx], masks["mask_connect"][sample_idx])
                    probs_spawn = self._get_masked_probs(logits_spawn[sample_idx], masks["mask_spawn"][sample_idx])

                    # [ì¶”ê°€] ì›ë³¸ ì ìˆ˜(Score) ê³„ì‚° (Softmax ì „ ë‹¨ê³„ì˜ ê°’)
                    # Score = Tanh(Logit) * Clipping_Value (ì˜ˆ: -10 ~ +10 ì‚¬ì´)
                    scores_type = self.logit_clipping * torch.tanh(logits_type[sample_idx]) # [ì¶”ê°€] Type ì ìˆ˜
                    scores_connect = self.logit_clipping * torch.tanh(logits_connect[sample_idx])
                    scores_spawn = self.logit_clipping * torch.tanh(logits_spawn[sample_idx])

                    # [ì¶”ê°€] í´ë¦¬í•‘ ì „ ì›ë³¸ ë¡œì§“(Raw Logit) ì¶”ì¶œ
                    raw_type = logits_type[sample_idx]
                    raw_connect = logits_connect[sample_idx]
                    raw_spawn = logits_spawn[sample_idx]

                    # --- 2. ì´ë¦„ ë§¤í•‘ ì¤€ë¹„ ---
                    # (í™˜ê²½ ì„¤ì •ì—ì„œ ì •ì  ì´ë¦„ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°)
                    node_names = env.generator.config.node_names
                    # [ìˆ˜ì •] ì›ë³¸ í…œí”Œë¦¿ì„ ì¶”ì í•˜ì—¬ ì´ë¦„ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
                    def get_name_with_origin(idx):

                        # 1. í…ì„œì—ì„œ ì‹¤ì œ ë…¸ë“œ íƒ€ì… ë° ì •ë³´ í™•ì¸
                        node_feat = td["nodes"][sample_idx, idx]
                        node_type = node_feat[FEATURE_INDEX["node_type"][0]:FEATURE_INDEX["node_type"][1]].argmax().item()

                        # 2. Battery
                        if node_type == NODE_TYPE_BATTERY:
                            return "BATTERY"
                        
                        # 3. Load (ëœë¤ ìƒì„±ëœ ìŠ¤í™ í‘œì‹œ)
                        if node_type == NODE_TYPE_LOAD:
                            v = node_feat[FEATURE_INDEX["vin_min"]].item()
                            i = node_feat[FEATURE_INDEX["current_active"]].item()
                            return f"RandomLoad_{idx} ({v:.1f}V, {i:.2f}A)"
                        
                        # 4. IC (Template) - ì •ì  ë¦¬ìŠ¤íŠ¸ ë§¤í•‘
                        if node_type == NODE_TYPE_IC:
                            # í˜„ì¬ ë°°ì¹˜ì˜ ì‹¤ì œ Load ê°œìˆ˜ ê³„ì‚°
                            node_types_all = td["nodes"][sample_idx, :, FEATURE_INDEX["node_type"][0]:FEATURE_INDEX["node_type"][1]].argmax(-1)
                            num_current_loads = (node_types_all == NODE_TYPE_LOAD).sum().item()

                            # í˜„ì¬ ë…¸ë“œê°€ í…œí”Œë¦¿ ì„¹ì…˜(Battery + Loads ì´í›„)ì— ìˆëŠ”ì§€ í™•ì¸
                            ic_start_idx = 1 + num_current_loads
                            
                            # í…œí”Œë¦¿ì´ë¼ë©´ ì •ì  ë¦¬ìŠ¤íŠ¸ì—ì„œ ì´ë¦„ ì°¾ê¸°
                            if idx >= ic_start_idx:
                                # ì •ì  ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°: [Batt(1)] + [FixedLoads] + [Templates]
                                # ë™ì  í…ì„œ êµ¬ì¡°: [Batt(1)] + [RandomLoads] + [Templates]
                                # ë”°ë¼ì„œ Templates ë‚´ì—ì„œì˜ ìƒëŒ€ì  ìœ„ì¹˜(offset)ëŠ” ë™ì¼í•¨
                                offset = idx - ic_start_idx
                                
                                static_ic_start = 1 + env.generator.num_loads # (ì´ˆê¸° JSON ë¡œë“œ ê°œìˆ˜)
                                target_static_idx = static_ic_start + offset
                                
                                if 0 <= target_static_idx < len(node_names):
                                    return node_names[target_static_idx]

                        # 5. ê·¸ ì™¸ (Empty, Spawned ë“±)
                        return f"Node_{idx}"

                    head_name = get_name_with_origin(current_head) # [ìˆ˜ì •]

                    # ---------------------------------------------------------
                    # [ì¶”ê°€] ì²« ìŠ¤í…ì—ì„œ ìƒì„±ëœ ë¬¸ì œ(Load & Constraints) ëª…ì„¸ì„œ ì¶œë ¥
                    # ---------------------------------------------------------
                    if decoding_step == 1:
                        log_fn("="*80)
                        log_fn(f"ğŸ² Generated Problem Specification (Sample 0)")
                        log_fn("-" * 80)
                        
                        # 1. Scalar Constraints (Prompt)
                        # scalar_p: [Temp, MaxSleep, I_Margin, Therm_Margin] (env_generator ì°¸ì¡°)
                        scalar_feats = td["scalar_prompt_features"][sample_idx]
                        temp = scalar_feats[0].item()
                        max_sleep = scalar_feats[1].item()
                        log_fn(f"ğŸŒ¡ï¸ Conditions: Ambient Temp={temp:.1f}Â°C, Max Sleep Current={max_sleep*1000:.1f}mA")
                        
                        # 2. Sequence Constraints (Matrix Prompt)
                        mat_feats = td["matrix_prompt_features"][sample_idx]
                        seq_srcs, seq_dsts = torch.where(mat_feats > 0.5)
                        if len(seq_srcs) > 0:
                            log_fn(f"â›“ï¸ Power Sequences ({len(seq_srcs)} Constraints):")
                            for s, d in zip(seq_srcs, seq_dsts):
                                s_name = get_name_with_origin(s.item())
                                d_name = get_name_with_origin(d.item())
                                log_fn(f"   â–º {s_name}  -->  {d_name}")
                        else:
                            log_fn(f"â›“ï¸ Power Sequences: None")

                        # 3. Load List
                        log_fn("-" * 80)
                        log_fn(f"ğŸ“¦ Generated Loads List:")
                        
                        load_count = 0
                        for i in range(self.N_MAX):
                            # ë…¸ë“œ íƒ€ì… í™•ì¸
                            nf = td["nodes"][sample_idx, i]
                            nt = nf[FEATURE_INDEX["node_type"][0]:FEATURE_INDEX["node_type"][1]].argmax().item()
                            
                            if nt == NODE_TYPE_LOAD:
                                load_count += 1
                                v = nf[FEATURE_INDEX["vin_min"]].item()
                                i_act = nf[FEATURE_INDEX["current_active"]].item()
                                i_slp = nf[FEATURE_INDEX["current_sleep"]].item()
                                
                                rail_val = nf[FEATURE_INDEX["independent_rail_type"]].item()
                                ao_val = nf[FEATURE_INDEX["always_on_in_sleep"]].item()
                                
                                # íƒœê·¸ ì •ë³´ (Supplier, Path, AO ë“±)
                                tags = []
                                if rail_val == 1.0: tags.append("Supplier")
                                if rail_val == 2.0: tags.append("Path")
                                if ao_val == 1.0: tags.append("AO")
                                tag_str = f"[{', '.join(tags)}]" if tags else ""
                                
                                log_fn(f"   - [Idx {i:03d}] {v:.2f}V / {i_act:.2f}A (Sleep: {i_slp*1000:.2f}mA) {tag_str}")
                        
                        log_fn(f"   (Total {load_count} Loads)")
                        log_fn("="*80)
                    # ---------------------------------------------------------

                    
                    # =========================================================
                    # âœ¨ [ìˆ˜ì •] Rail Type + AO ìƒíƒœ ì •ë³´ ì¶”ì¶œ ë° ë¡œê·¸ í¬ë§· âœ¨                    # =========================================================
                    # 1. Rail Type (ë…ë¦½ ì—¬ë¶€)
                    rail_val = td["nodes"][sample_idx, current_head, FEATURE_INDEX["independent_rail_type"]].item()
                    # ê°€ë…ì„±ì„ ìœ„í•œ ë¬¸ìì—´ ë§¤í•‘ (0:Normal, 1:Sup, 2:Path)
                    if rail_val == 1.0: rail_str = "Type: Supplier(1)"
                    elif rail_val == 2.0: rail_str = "Type: Path(2)"
                    else: rail_str = "Type: Normal(0)"
                    
                    # 2. AO State (ì•”ì „ë¥˜ ìƒíƒœ)
                    ao_val = td["nodes"][sample_idx, current_head, FEATURE_INDEX["always_on_in_sleep"]].item()
                    ao_str = "AO: Yes" if ao_val == 1.0 else "AO: No"
                    
                    # (idxì™€ typeì„ í•¨ê»˜ ì¶œë ¥)
                    log_fn(f"\n[Step {decoding_step:02d}] Current Head: {head_name} (idx: {current_head} | {rail_str} | {ao_str})")
                    # =========================================================

                    # --- 3. Action Type í™•ë¥  ì¶œë ¥ ---
                    p_conn = probs_type[0].item()
                    p_spwn = probs_type[1].item()
                    
                    s_conn = scores_type[0].item()
                    s_spwn = scores_type[1].item()

                    r_conn = raw_type[0].item()
                    r_spwn = raw_type[1].item()

                    chosen_type = action_type[sample_idx].item()
                    type_str = "Connect" if chosen_type == 0 else "Spawn"
                    
                    is_connect_valid = masks["mask_type"][sample_idx, 0].item()
                    is_spawn_valid = masks["mask_type"][sample_idx, 1].item()

                    tag_conn = "" if is_connect_valid else " ğŸš« [Masked]"
                    tag_spwn = "" if is_spawn_valid else " ğŸš« [Masked]"
                    
                    log_fn(f"  ğŸ“Š Action Type Probabilities:")
                    log_fn(f"     - Connect: {p_conn*100:.2f}% (Sc: {s_conn:6.3f} | Raw: {r_conn:6.3f}){tag_conn} {'ğŸ‘ˆ Selected' if chosen_type==0 else ''}")
                    log_fn(f"     - Spawn  : {p_spwn*100:.2f}% (Sc: {s_spwn:6.3f} | Raw: {r_spwn:6.3f}){tag_spwn} {'ğŸ‘ˆ Selected' if chosen_type==1 else ''}")
                    # --- 4. ìƒì„¸ í›„ë³´ í™•ë¥  ì¶œë ¥ ---
                    
                    # (A) Connect í›„ë³´ë“¤
                    if masks["mask_type"][sample_idx, 0]: # Connectê°€ ê°€ëŠ¥í•œ ê²½ìš°ë§Œ
                        log_fn(f"  ğŸ”— Connect Candidates (P(Target | Connect)):")
                        valid_connect_indices = torch.where(masks["mask_connect"][sample_idx])[0]
                        
                        # í™•ë¥ ìˆœ ì •ë ¬
                        cand_probs = []
                        for idx in valid_connect_indices:
                            i = idx.item()
                            prob = probs_connect[i].item()
                            score = scores_connect[i].item()
                            raw = raw_connect[i].item()
                            
                            # Connect ë§ˆìŠ¤í¬ í™•ì¸ (ë””ë²„ê¹…ìš©)
                            is_valid = masks["mask_connect"][sample_idx, i].item()
                            tag = "" if is_valid else " ğŸš« [Masked] (Error?)"
                            
                            cand_probs.append((prob, score, raw, i, tag))                            
                        cand_probs.sort(key=lambda x: x[0], reverse=True)

                        # [ìˆ˜ì •] 5ê°œ í•­ëª© ì–¸íŒ¨í‚¹ (tag í¬í•¨)
                        for prob, score, raw, idx, tag in cand_probs:
                            name = get_name_with_origin(idx)
                            is_picked = (chosen_type == 0 and action_connect[sample_idx].item() == idx)
                            # ì´ë¦„ ê³µê°„ì„ 25 -> 60ìœ¼ë¡œ ëŠ˜ë¦¼ (ê¸´ ì´ë¦„ í‘œì‹œìš©)
                            log_fn(f"     - {name:<60} : {prob*100:.2f}% (Sc: {score:6.3f} | Raw: {raw:6.3f}){tag} {'âœ…' if is_picked else ''}")

                    if masks["mask_type"][sample_idx, 1]: # Spawnì´ ê°€ëŠ¥í•œ ê²½ìš°ë§Œ
                        log_fn(f"  ğŸ“¦ Spawn Candidates (P(Template | Spawn)):")
                        valid_spawn_indices = torch.where(masks["mask_spawn"][sample_idx])[0]
                        
                        cand_probs = []
                        for idx in valid_spawn_indices:
                            i = idx.item()
                            prob = probs_spawn[i].item()
                            score = scores_spawn[i].item()
                            raw = raw_spawn[i].item()

                            is_valid = masks["mask_spawn"][sample_idx, i].item()
                            tag = "" if is_valid else " ğŸš« [Masked] (Error?)"

                            cand_probs.append((prob, score, raw, i, tag))

                        cand_probs.sort(key=lambda x: x[0], reverse=True)

                        for prob, score, raw, idx, tag in cand_probs:
                            name = get_name_with_origin(idx)
                            is_picked = (chosen_type == 1 and action_spawn[sample_idx].item() == idx)
                            log_fn(f"     - {name:<60} : {prob*100:.2f}% (Sc: {score:6.3f} | Raw: {raw:6.3f}){tag} {'âœ…' if is_picked else ''}")

                    log_fn("-" * 60)
            # [END]: 'detail' ëª¨ë“œ ì•¡ì…˜ ë¡œê¹…

            # 6. í™˜ê²½ ìŠ¤í… ì‹¤í–‰
            with torch.no_grad():
                td.set("action", action_dict)
                output_td = env.step(td)
            
            reward = output_td["reward"]
            td = output_td["next"]
            
            # 7. A2C í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘
            log_probs.append(final_log_prob)
            actions.append(action_dict)
            rewards.append(reward)

        # 8. ìµœì¢… ê²°ê³¼ ì·¨í•©
        if not rewards:
            # (ë””ì½”ë”© ë£¨í”„ê°€ 1ë²ˆë„ ëŒì§€ ì•Šì€ ê²½ìš° - ì˜ˆ: ì´ë¯¸ ì™„ë£Œëœ ìƒíƒœ)
            B_total = td.batch_size[0]
            dummy_reward = torch.zeros(B_total, 1, device=td.device)
            dummy_log_prob = torch.zeros(B_total, device=td.device)
            dummy_value = torch.zeros(B_total, 1, device=td.device)
            return {
                "reward": dummy_reward,
                "log_likelihood": dummy_log_prob,
                "actions": [],
                "value": dummy_value,
            }

        # (B_total, T) -> (B_total, 1)
        total_reward = torch.stack(rewards, 1).sum(1)
        # (B_total, T) -> (B_total)
        total_log_likelihood = torch.stack(log_probs, 1).sum(1)

        # [ì¶”ê°€] í‰ê·  ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
        if entropies:
            avg_entropy = torch.stack(entropies, 1).mean(1) # (B,) ì—í”¼ì†Œë“œ í‰ê· 
        else:
            avg_entropy = torch.zeros_like(total_log_likelihood)

        # [ì¶”ê°€] ìµœì¢… ìƒíƒœì—ì„œ ë¹„ìš© ì •ë³´ ì¶”ì¶œ
        final_bom_cost = td["current_cost"].squeeze(-1)
        final_sleep_cost = td["sleep_cost"].squeeze(-1)


        result = {
            "reward": total_reward,
            "log_likelihood": total_log_likelihood,
            "entropy": avg_entropy, # [ì¶”ê°€]
            "actions": actions,  # (ë””ë²„ê¹…ìš©)
            "value": first_value,
            "bom_cost": final_bom_cost, # [ì¶”ê°€]
            "sleep_cost": final_sleep_cost, # [ì¶”ê°€]
        }

        if return_final_td:
            # ì‹œê°í™”/ë””ë²„ê¹…ìš© ìµœì¢… ìƒíƒœëŠ” GPU ì „ì²´ TensorDictë¥¼ í†µì§¸ë¡œ
            # clone() í•˜ëŠ” ëŒ€ì‹ ,
            #  - ê·¸ë˜ë””ì–¸íŠ¸ ì—°ê²°ì„ ëŠê³ (detach)
            #  - í•„ìš”í•œ í‚¤ë§Œ ê³¨ë¼ì„œ
            #  - CPU ë©”ëª¨ë¦¬ë¡œë§Œ ì €ì¥í•œë‹¤.
            #
            # visualize_result()ì—ì„œ ì‚¬ìš©í•˜ëŠ” í‚¤:
            #   - "nodes"
            #   - "adj_matrix"
            #   - "is_active_mask"
            final_td_cpu = TensorDict(
                {
                    "nodes": td["nodes"].detach().cpu(),
                    "adj_matrix": td["adj_matrix"].detach().cpu(),
                    "is_active_mask": td["is_active_mask"].detach().cpu(),
                },
                batch_size=td.batch_size,
            )
            result["final_td"] = final_td_cpu


        return result